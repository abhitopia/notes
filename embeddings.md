Pre-trained Embedding vs Learning Embedding implicitly
- Generating word embeddings with a very deep architecture is simply too computationally expensive for a large vocabulary
- training objective: word2vec and GloVe are geared towards producing word embeddings that encode general semantic relationships, 
which are beneficial to many downstream tasks
- common weight initialisation that provides generally helpful features
