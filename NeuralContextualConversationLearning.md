
- Improvements over Seq2Seq to improve diversity of responses.
- Uses additional memory mechanism
- Challenges with conventional systems
   - Context sensitivity
   - scalablity
   - robustness

- Literature Review
   - Seq2seq used for English to French translation (2014)
   - Attention Mechanism on bi-RNN (2015) produced SOA on translation
   - Same seq2seq on movie dialogues (2015) produces generic replies like "of course"
   - Recent works suggest improvement by 
       - encoding previous utterance as additional inputs
       - optimize mutual information instead of cross entropy



